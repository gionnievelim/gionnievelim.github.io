<!DOCTYPE html>

<html>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1"> <!--https://stackoverflow.com/a/30709473-->
  <title>Perceptions of Explanations in Automated Fact-Checkers</title>
  <link rel="stylesheet" href="/assets/css/styles.css" />
  <link type="application/atom+xml" rel="alternate" href="/feed.xml" />
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Perceptions of Explanations in Automated Fact-Checkers</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Perceptions of Explanations in Automated Fact-Checkers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Automated misinformation detection is commonly used in social media apps to surveil the chaotic and massive information space. Measures taken when misinformation is identifed include fagging, blocking, and removing of the content. When fagged, a fact-checking professional is alerted by the system to manually vet the content. When removed, users may see a label overlay on the content. In both cases, humans are at the receiving end of the action, yet opaque artifcial intelligence systems are unable to provide information on how the decision was made, making the decision questionable and doubtful. Explainable artifcial intelligence (XAI) has been proposed to address this. In considering the use of XAI for automated fact-checking, we discuss the rationale and design of a comparative study that seeks to understand users’ preferences and behaviors towards different explanations used in automated fact-checking with the goal of identifying design considerations." />
<meta property="og:description" content="Automated misinformation detection is commonly used in social media apps to surveil the chaotic and massive information space. Measures taken when misinformation is identifed include fagging, blocking, and removing of the content. When fagged, a fact-checking professional is alerted by the system to manually vet the content. When removed, users may see a label overlay on the content. In both cases, humans are at the receiving end of the action, yet opaque artifcial intelligence systems are unable to provide information on how the decision was made, making the decision questionable and doubtful. Explainable artifcial intelligence (XAI) has been proposed to address this. In considering the use of XAI for automated fact-checking, we discuss the rationale and design of a comparative study that seeks to understand users’ preferences and behaviors towards different explanations used in automated fact-checking with the goal of identifying design considerations." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-04-23T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Perceptions of Explanations in Automated Fact-Checkers" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-04-23T00:00:00+08:00","datePublished":"2022-04-23T00:00:00+08:00","description":"Automated misinformation detection is commonly used in social media apps to surveil the chaotic and massive information space. Measures taken when misinformation is identifed include fagging, blocking, and removing of the content. When fagged, a fact-checking professional is alerted by the system to manually vet the content. When removed, users may see a label overlay on the content. In both cases, humans are at the receiving end of the action, yet opaque artifcial intelligence systems are unable to provide information on how the decision was made, making the decision questionable and doubtful. Explainable artifcial intelligence (XAI) has been proposed to address this. In considering the use of XAI for automated fact-checking, we discuss the rationale and design of a comparative study that seeks to understand users’ preferences and behaviors towards different explanations used in automated fact-checking with the goal of identifying design considerations.","headline":"Perceptions of Explanations in Automated Fact-Checkers","mainEntityOfPage":{"@type":"WebPage","@id":"/publications/2022-04-23-chi2022workshop.html"},"url":"/publications/2022-04-23-chi2022workshop.html"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body>
  <div id="content">
    <p>Perceptions of Explanations in Automated Fact-Checkers</p>
<p>Automated misinformation detection is commonly used in social media apps to surveil the chaotic and massive information space. Measures taken when misinformation is identifed include fagging, blocking, and removing of the content. When fagged, a fact-checking professional is alerted by the system to manually vet the content. When removed, users may see a label overlay on the content. In both cases, humans are at the receiving end of the action, yet opaque artifcial intelligence systems are unable to provide information on how the decision was made, making the decision questionable and doubtful. Explainable artifcial intelligence (XAI) has been proposed to address this. In considering the use of XAI for automated fact-checking, we discuss the rationale and design of a comparative study that seeks to understand users’ preferences and behaviors towards different explanations used in automated fact-checking with the goal of identifying design considerations.</p>

<p><a href="https://www.dropbox.com/s/dwoxp7qmnj145te/HCXAI2022_paper_16.pdf?dl=0">Access the Paper</a></p>

<p>CHI 2022 Workshop on Human-Centered Explainable AI (HCXAI)</p>
<p>April 2022</p>
  </div>

  <nav>
  
  <a href="/" >Home</a>
  
  <a href="/research/" >Research</a>
  
  <a href="/contact/" >Contact</a>
  
</nav>

  <footer>
    © 2022 Gionnieve Lim<!--. All rights reserved.-->
  </footer>
</body>

</html>