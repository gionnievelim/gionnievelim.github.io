<!DOCTYPE html>

<html>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1"> <!--https://stackoverflow.com/a/30709473-->
  <title>Explanation Preferences in XAI Fact-Checkers</title>
  <link rel="stylesheet" href="/assets/css/styles.css" />
  <link type="application/atom+xml" rel="alternate" href="/feed.xml" />
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Explanation Preferences in XAI Fact-Checkers</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Explanation Preferences in XAI Fact-Checkers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="As misinformation grows rampantly, fact-checking has become an inordinate task that calls for automation. While there has been much advancement in the identification of misinformation using artificial intelligence (AI), these systems tend to be opaque, fulfilling little of what fact-checking does to convince users of its evaluation. A proposition for this is the use of explainable AI (XAI) to reveal the decision-making processes of the AI. As research on XAI fact-checkers accumulate, investigating user attitudes on the use of AI in fact-checking and towards different styles of explanations will contribute to an understanding of explanation preferences in XAI fact-checkers. We present the preliminary results of a perception study with 22 participants, finding a clear preference towards explanations mimicking organic fact-checking practices and towards explanations that use texts or that contain more details. These early findings may guide the design of XAI to enhance the performance of the human-AI system." />
<meta property="og:description" content="As misinformation grows rampantly, fact-checking has become an inordinate task that calls for automation. While there has been much advancement in the identification of misinformation using artificial intelligence (AI), these systems tend to be opaque, fulfilling little of what fact-checking does to convince users of its evaluation. A proposition for this is the use of explainable AI (XAI) to reveal the decision-making processes of the AI. As research on XAI fact-checkers accumulate, investigating user attitudes on the use of AI in fact-checking and towards different styles of explanations will contribute to an understanding of explanation preferences in XAI fact-checkers. We present the preliminary results of a perception study with 22 participants, finding a clear preference towards explanations mimicking organic fact-checking practices and towards explanations that use texts or that contain more details. These early findings may guide the design of XAI to enhance the performance of the human-AI system." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-04-21T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Explanation Preferences in XAI Fact-Checkers" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-04-21T00:00:00+08:00","datePublished":"2022-04-21T00:00:00+08:00","description":"As misinformation grows rampantly, fact-checking has become an inordinate task that calls for automation. While there has been much advancement in the identification of misinformation using artificial intelligence (AI), these systems tend to be opaque, fulfilling little of what fact-checking does to convince users of its evaluation. A proposition for this is the use of explainable AI (XAI) to reveal the decision-making processes of the AI. As research on XAI fact-checkers accumulate, investigating user attitudes on the use of AI in fact-checking and towards different styles of explanations will contribute to an understanding of explanation preferences in XAI fact-checkers. We present the preliminary results of a perception study with 22 participants, finding a clear preference towards explanations mimicking organic fact-checking practices and towards explanations that use texts or that contain more details. These early findings may guide the design of XAI to enhance the performance of the human-AI system.","headline":"Explanation Preferences in XAI Fact-Checkers","mainEntityOfPage":{"@type":"WebPage","@id":"/publications/2022-04-21-ecscw2022poster.html"},"url":"/publications/2022-04-21-ecscw2022poster.html"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body>
  <div id="content">
    <p>Explanation Preferences in XAI Fact-Checkers</p>
<p>As misinformation grows rampantly, fact-checking has become an inordinate task that calls for automation. While there has been much advancement in the identification of misinformation using artificial intelligence (AI), these systems tend to be opaque, fulfilling little of what fact-checking does to convince users of its evaluation. A proposition for this is the use of explainable AI (XAI) to reveal the decision-making processes of the AI. As research on XAI fact-checkers accumulate, investigating user attitudes on the use of AI in fact-checking and towards different styles of explanations will contribute to an understanding of explanation preferences in XAI fact-checkers. We present the preliminary results of a perception study with 22 participants, finding a clear preference towards explanations mimicking organic fact-checking practices and towards explanations that use texts or that contain more details. These early findings may guide the design of XAI to enhance the performance of the human-AI system.</p>

<!-- <a href="https://doi.org/10.48340/ecscw2022_p02">Access the Paper</a> -->

<p>ECSCW 2022 Posters</p>
<p>April 2022</p>
  </div>

  <nav>
  
  <a href="/" >Home</a>
  
  <a href="/research/" >Research</a>
  
  <a href="/contact/" >Contact</a>
  
</nav>

  <footer>
    Â© 2022 Gionnieve Lim<!--. All rights reserved.-->
  </footer>
</body>

</html>