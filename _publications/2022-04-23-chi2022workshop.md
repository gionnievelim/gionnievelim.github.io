---
layout: publication
date: 2022-04-23
categories: workshop
type: "CHI 2022 Workshop Position Paper"
title: "Perceptions of Explanations in Automated Fact-Checkers"
venue: "CHI 2022 Workshop on Human-Centered Explainable AI (HCXAI)"
link: "https://www.dropbox.com/s/dwoxp7qmnj145te/HCXAI2022_paper_16.pdf?dl=0"
---

Automated misinformation detection is commonly used in social media apps to surveil the chaotic and massive information space. Measures taken when misinformation is identifed include fagging, blocking, and removing of the content. When fagged, a fact-checking professional is alerted by the system to manually vet the content. When removed, users may see a label overlay on the content. In both cases, humans are at the receiving end of the action, yet opaque artifcial intelligence systems are unable to provide information on how the decision was made, making the decision questionable and doubtful. Explainable artifcial intelligence (XAI) has been proposed to address this. In considering the use of XAI for automated fact-checking, we discuss the rationale and design of a comparative study that seeks to understand usersâ€™ preferences and behaviors towards different explanations used in automated fact-checking with the goal of identifying design considerations.

<a href="https://www.dropbox.com/s/dwoxp7qmnj145te/HCXAI2022_paper_16.pdf?dl=0">Access the Paper</a>